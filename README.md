Äá» tÃ i: TÄ‚NG Tá»C PHÃ‰P NHÃ‚N MA TRáº¬N Sá»¬ Dá»¤NG Ká»¸ THUáº¬T CHIA KHá»I (TILED) Vá»šI CUDA 
TRIá»‚N KHAI NHÃ‚N MA TRáº¬N SONG SONG Báº°NG CUDA Sá»¬ Dá»¤NG Ká»¸ THUáº¬T CHIA KHá»I 

Tá»•ng quan vá» giáº£i phÃ¡p 

Giáº£i phÃ¡p nhÃ¢n ma tráº­n song song Ä‘Æ°á»£c triá»ƒn khai báº±ng cÃ¡ch sá»­ dá»¥ng kiáº¿n trÃºc CUDA cá»§a NVIDIA, táº­n dá»¥ng kháº£ nÄƒng tÃ­nh toÃ¡n song song cá»§a GPU. Ã tÆ°á»Ÿng chÃ­nh lÃ  chia ma tráº­n káº¿t quáº£ P thÃ nh cÃ¡c khá»‘i (tiles) nhá» hÆ¡n, vÃ  má»—i khá»‘i luá»“ng (thread block) trÃªn GPU sáº½ chá»‹u trÃ¡ch nhiá»‡m tÃ­nh toÃ¡n má»™t khá»‘i P nÃ y. Ká»¹ thuáº­t chia khá»‘i (tiling) Ä‘Æ°á»£c Ã¡p dá»¥ng báº±ng cÃ¡ch sá»­ dá»¥ng bá»™ nhá»› chia sáº» (shared memory) Ä‘á»ƒ lÆ°u trá»¯ táº¡m thá»i cÃ¡c khá»‘i con cá»§a ma tráº­n Ä‘áº§u vÃ o M vÃ  N, nháº±m giáº£m thiá»ƒu sá»‘ láº§n truy cáº­p vÃ o bá»™ nhá»› toÃ n cá»¥c (global memory) cháº­m cháº¡p vÃ  tÄƒng cÆ°á»ng tÃ¡i sá»­ dá»¥ng dá»¯ liá»‡u. 

CÃ¡ch Ã¡nh xáº¡ bÃ i toÃ¡n lÃªn kiáº¿n trÃºc CUDA (thread, block, grid): 

Grid:â€¯ToÃ n bá»™ ma tráº­n káº¿t quáº£ P Ä‘Æ°á»£c Ã¡nh xáº¡ vÃ o má»™t grid 2 chiá»u cÃ¡c khá»‘i luá»“ng. Sá»‘ lÆ°á»£ng khá»‘i luá»“ng theo chiá»u x cá»§a grid (gridSize.x) Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch láº¥y tráº§n cá»§a (chiá»u rá»™ng P /â€¯TILE_WIDTH), vÃ  tÆ°Æ¡ng tá»± cho chiá»u y (gridSize.yâ€¯= tráº§n cá»§a (chiá»u cao P /â€¯TILE_WIDTH)). 

Block:â€¯Má»—i khá»‘i luá»“ng (thread block) lÃ  má»™t ma tráº­n 2 chiá»u cÃ¡c luá»“ng, cÃ³ kÃ­ch thÆ°á»›câ€¯TILE_WIDTHâ€¯xâ€¯TILE_WIDTH. Má»—i khá»‘i luá»“ng nÃ y chá»‹u trÃ¡ch nhiá»‡m tÃ­nh toÃ¡n má»™t khá»‘i (tile) tÆ°Æ¡ng á»©ng cÃ³ kÃ­ch thÆ°á»›câ€¯TILE_WIDTHâ€¯xâ€¯TILE_WIDTHâ€¯trong ma tráº­n káº¿t quáº£ P. 

Thread:â€¯Má»—i luá»“ng (thread) trong má»™t khá»‘i luá»“ng chá»‹u trÃ¡ch nhiá»‡m tÃ­nh toÃ¡n má»™t pháº§n tá»­ duy nháº¥t trong khá»‘i P mÃ  khá»‘i luá»“ng Ä‘Ã³ quáº£n lÃ½. Tá»a Ä‘á»™ cá»§a luá»“ng trong khá»‘i (threadIdx.x,â€¯threadIdx.y) káº¿t há»£p vá»›i tá»a Ä‘á»™ cá»§a khá»‘i trong grid (blockIdx.x,â€¯blockIdx.y) vÃ â€¯TILE_WIDTHâ€¯sáº½ xÃ¡c Ä‘á»‹nh pháº§n tá»­ P[row][column] mÃ  luá»“ng Ä‘Ã³ tÃ­nh toÃ¡n. 

Vai trÃ² cá»§a shared memory trong viá»‡c lÆ°u trá»¯ cÃ¡c khá»‘i (tile): 

Bá»™ nhá»› chia sáº» (__shared__) Ä‘Ã³ng vai trÃ² cá»±c ká»³ quan trá»ng. Thay vÃ¬ má»—i luá»“ng Ä‘á»c trá»±c tiáº¿p cÃ¡c pháº§n tá»­ tá»« ma tráº­n M vÃ  N trong global memory cho má»—i phÃ©p nhÃ¢n, cÃ¡c luá»“ng trong má»™t khá»‘i sáº½ há»£p tÃ¡c Ä‘á»ƒ táº£i cÃ¡c khá»‘i (tiles) tÆ°Æ¡ng á»©ng cá»§a M vÃ  N vÃ o hai máº£ng shared memory (tileMsâ€¯vÃ â€¯tileNs). Sau Ä‘Ã³, cÃ¡c phÃ©p nhÃ¢n vÃ  cá»™ng dá»“n Ä‘á»ƒ tÃ­nh toÃ¡n cÃ¡c pháº§n tá»­ cá»§a khá»‘i P sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n chá»§ yáº¿u báº±ng cÃ¡ch truy cáº­p dá»¯ liá»‡u tá»« shared memory, vá»‘n nhanh hÆ¡n nhiá»u so vá»›i global memory. Äiá»u nÃ y giÃºp: 

Giáº£m truy cáº­p Global Memory:â€¯Má»—i pháº§n tá»­ trong cÃ¡c khá»‘i cá»§a M vÃ  N chá»‰ cáº§n Ä‘Æ°á»£c táº£i tá»« global memory vÃ o shared memory má»™t láº§n cho má»—i "phase" tÃ­nh toÃ¡n khá»‘i P. 

TÄƒng tÃ¡i sá»­ dá»¥ng dá»¯ liá»‡u:â€¯Dá»¯ liá»‡u trong shared memory Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng nhiá»u láº§n bá»Ÿi cÃ¡c luá»“ng trong cÃ¹ng má»™t khá»‘i. 

 

HÃ m Kernel nhÃ¢n ma tráº­n chia khá»‘i (MatrixMulKernel) 

ÄÃ¢y lÃ  trÃ¡i tim cá»§a viá»‡c tÃ­nh toÃ¡n song song trÃªn GPU. 

 

HÃ¬nh 4.1 SÆ¡ Ä‘á»“ thuáº­t toÃ¡n song song pháº§n Device 

Khai bÃ¡o kernel (__global__ void MatrixMulKernel(Matrix M, Matrix N, Matrix P)): 

Tá»« khÃ³aâ€¯__global__â€¯chá»‰ Ä‘á»‹nh ráº±ngâ€¯MatrixMulKernelâ€¯lÃ  má»™t hÃ m kernel sáº½ Ä‘Æ°á»£c thá»±c thi trÃªn GPU vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c gá»i tá»« mÃ£ host (CPU). HÃ m nháº­n vÃ o ba Ä‘á»‘i tÆ°á»£ng kiá»ƒuâ€¯Matrixâ€¯(Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trongâ€¯matrixmul.h, cÃ³ láº½ chá»©a con trá»â€¯elements,â€¯width,â€¯height,â€¯pitch) Ä‘áº¡i diá»‡n cho ma tráº­n M, N vÃ  ma tráº­n káº¿t quáº£ P. 

XÃ¡c Ä‘á»‹nh chá»‰ sá»‘ thread vÃ  block: 

â€œint tx = threadIdx.x; int ty = threadIdx.y; 

              int bx = blockIdx.x; int by = blockIdx.y;â€ 

CÃ¡c biáº¿n nÃ y lÆ°u trá»¯ chá»‰ sá»‘ cá»§a luá»“ng hiá»‡n táº¡i trong khá»‘i (tx,â€¯ty) vÃ  chá»‰ sá»‘ cá»§a khá»‘i hiá»‡n táº¡i trong grid (bx,â€¯by). 

Cáº¥p phÃ¡t vÃ  sá»­ dá»¥ng shared memory: 

â€œ__shared__ float tileMs[TILE_WIDTH][TILE_WIDTH]; 

  __shared__ float tileNs[TILE_WIDTH][TILE_WIDTH];â€ 

Hai máº£ng 2 chiá»uâ€¯tileMsâ€¯vÃ â€¯tileNsâ€¯Ä‘Æ°á»£c khai bÃ¡o vá»›i tá»« khÃ³aâ€¯__shared__. KÃ­ch thÆ°á»›c cá»§a chÃºng lÃ â€¯TILE_WIDTHâ€¯xâ€¯TILE_WIDTH. Má»—i khá»‘i luá»“ng sáº½ cÃ³ má»™t báº£n sao riÃªng cá»§a hai máº£ng nÃ y. ChÃºng sáº½ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c khá»‘i (tiles) cá»§a M vÃ  N.â€¯TILE_WIDTHâ€¯Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a sáº½ thay Ä‘á»•i tÃ¹y theo lÃºc test. 

QuÃ¡ trÃ¬nh táº£i cÃ¡c khá»‘i (tile) tá»« global memory vÃ o shared memory: 

Trong vÃ²ng láº·pâ€¯for(int i=0; i<ceilf(M.width/(float)TILE_WIDTH); i++): 
Má»—i luá»“ng trong khá»‘i (ty,â€¯tx) chá»‹u trÃ¡ch nhiá»‡m táº£i má»™t pháº§n tá»­ tá»« global memory cá»§a M vÃ  N vÃ o vá»‹ trÃ­ tÆ°Æ¡ng á»©ng trongâ€¯tileMsâ€¯vÃ â€¯tileNs. 

â€œint row = by * TILE_WIDTH + ty; 

int column = bx * TILE_WIDTH + tx; 

for(int i=0;i<ceilf(M.width/(float)TILE_WIDTH);i++){ 

if(row < M.height && (i*TILE_WIDTH + tx)<M.width)  // (A) Kiá»ƒm tra biÃªn cho M 

    tileMs[ty][tx] = M.elements[row*M.width + i*TILE_WIDTH + tx];     // (B) Táº£i pháº§n tá»­ M vÃ o shared memory 

else 

    tileMs[ty][tx] = 0; // (C) GÃ¡n 0 náº¿u ngoÃ i biÃªn 

if(column < N.width && (i*TILE_WIDTH + ty)<N.height) // (D) Kiá»ƒm tra biÃªn cho N 

    tileNs[ty][tx] = N.elements[(i*TILE_WIDTH + ty)*N.width + column]; // (E) Táº£i pháº§n tá»­ N vÃ o shared memory 

else 

    tileNs[ty][tx] = 0; // (F) GÃ¡n 0 náº¿u ngoÃ i biÃªnâ€ 

(A) & (D):â€¯CÃ¡c Ä‘iá»u kiá»‡nâ€¯ifâ€¯kiá»ƒm tra xem tá»a Ä‘á»™ truy cáº­p cÃ³ náº±m trong giá»›i háº¡n cá»§a ma tráº­n M vÃ  N hay khÃ´ng. Äiá»u nÃ y quan trá»ng khi kÃ­ch thÆ°á»›c ma tráº­n khÃ´ng chia háº¿t choâ€¯TILE_WIDTH. 

(B):â€¯Luá»“ngâ€¯(ty, tx)â€¯táº£i pháº§n tá»­â€¯M[row][i*TILE_WIDTH + tx]â€¯vÃ oâ€¯tileMs[ty][tx].â€¯rowâ€¯lÃ  hÃ ng toÃ n cá»¥c cá»§a M mÃ  luá»“ng nÃ y liÃªn quan (do khá»‘i luá»“ng Ä‘ang tÃ­nh khá»‘i P táº¡i hÃ ngâ€¯by), vÃ â€¯i*TILE_WIDTH + txâ€¯lÃ  cá»™t toÃ n cá»¥c cá»§a M. 

(E):â€¯Luá»“ngâ€¯(ty, tx)â€¯táº£i pháº§n tá»­â€¯N[i*TILE_WIDTH + ty][column]â€¯vÃ oâ€¯tileNs[ty][tx].â€¯i*TILE_WIDTH + tyâ€¯lÃ  hÃ ng toÃ n cá»¥c cá»§a N, vÃ â€¯columnâ€¯lÃ  cá»™t toÃ n cá»¥c cá»§a N mÃ  luá»“ng nÃ y liÃªn quan. LÆ°u Ã½ ráº±ng cÃ¡ch cÃ¡c luá»“ng táº£iâ€¯tileNsâ€¯cÃ³ váº» nhÆ° Ä‘ang táº£i má»™t cá»™t cá»§aâ€¯Nâ€¯vÃ oâ€¯tileNsâ€¯theo cÃ¡ch mÃ  má»—i luá»“ng táº£i má»™t pháº§n tá»­ cá»§a cá»™t Ä‘Ã³. 

(C) & (F):â€¯Náº¿u tá»a Ä‘á»™ náº±m ngoÃ i biÃªn, giÃ¡ trá»‹ 0 Ä‘Æ°á»£c gÃ¡n vÃ o shared memory Ä‘á»ƒ khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ nhÃ¢n. 

 

â€¯Äá»“ng bá»™ hÃ³a cÃ¡c thread trong block (__syncthreads()): 

â€œ__syncthreads(); // (G) 

â€¦ 

              __syncthreads(); // (H)â€ 

(G):â€¯Sau khi táº¥t cáº£ cÃ¡c luá»“ng trong khá»‘i Ä‘Ã£ cá»‘ gáº¯ng táº£i xong pháº§n tá»­ cá»§a mÃ¬nh vÃ oâ€¯tileMsâ€¯vÃ â€¯tileNsâ€¯cho "phase"â€¯iâ€¯hiá»‡n táº¡i,â€¯__syncthreads()â€¯Ä‘Æ°á»£c gá»i. Lá»‡nh nÃ y Ä‘áº£m báº£o ráº±ngâ€¯táº¥t cáº£ cÃ¡c luá»“ng trong khá»‘i pháº£i Ä‘á»£i nhau táº¡i Ä‘Ã¢yâ€¯cho Ä‘áº¿n khi toÃ n bá»™ dá»¯ liá»‡u cá»§a hai khá»‘i con Ä‘Ã£ Ä‘Æ°á»£c náº¡p hoÃ n chá»‰nh vÃ o shared memory trÆ°á»›c khi báº¥t ká»³ luá»“ng nÃ o báº¯t Ä‘áº§u thá»±c hiá»‡n phÃ©p nhÃ¢n. 

(H):â€¯Sau khi vÃ²ng láº·pâ€¯jâ€¯(tÃ­nh toÃ¡n trÃªn shared memory) káº¿t thÃºc,â€¯__syncthreads()â€¯Ä‘Æ°á»£c gá»i láº¡i. Lá»‡nh nÃ y Ä‘áº£m báº£o ráº±ng táº¥t cáº£ cÃ¡c luá»“ng Ä‘Ã£ hoÃ n thÃ nh viá»‡c sá»­ dá»¥ng dá»¯ liá»‡u trongâ€¯tileMsâ€¯vÃ â€¯tileNsâ€¯cá»§a "phase"â€¯iâ€¯hiá»‡n táº¡i, trÆ°á»›c khi khá»‘i luá»“ng chuyá»ƒn sang "phase"â€¯i+1â€¯(tá»©c lÃ  táº£i cÃ¡c khá»‘i con tiáº¿p theo cá»§a M vÃ  N). 

 

Thá»±c hiá»‡n phÃ©p nhÃ¢n cÃ¡c ma tráº­n con trong shared memory: 

â€œfloat pValue = 0; 

... 

for(int j=0;j<TILE_WIDTH;j++) 

    pValue += tileMs[ty][j] * tileNs[j][tx]; // (I)â€ 

(I):â€¯Sau khiâ€¯__syncthreads()â€¯(G) Ä‘áº£m báº£o dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng trong shared memory, má»—i luá»“ng thá»±c hiá»‡n tÃ­nh toÃ¡n má»™t pháº§n cá»§a tÃ­ch vÃ´ hÆ°á»›ng. Luá»“ngâ€¯(ty, tx)â€¯tÃ­nhâ€¯pValueâ€¯(sáº½ lÃ  má»™t pháº§n tá»­ cá»§a khá»‘i P mÃ  nÃ³ phá»¥ trÃ¡ch) báº±ng cÃ¡ch nhÃ¢n hÃ ngâ€¯tyâ€¯cá»§aâ€¯tileMsâ€¯vá»›i cá»™tâ€¯txâ€¯cá»§aâ€¯tileNsâ€¯vÃ  cá»™ng dá»“n káº¿t quáº£. ÄÃ¢y chÃ­nh lÃ  phÃ©p nhÃ¢n ma tráº­n giá»¯a hai khá»‘i conâ€¯tileMsâ€¯vÃ â€¯tileNsâ€¯Ä‘ang náº±m trong shared memory. VÃ²ng láº·p nÃ y cháº¡yâ€¯TILE_WIDTHâ€¯láº§n. 

 

Ghi káº¿t quáº£ tá»« registers ra global memory: 

â€œif(row < P.height && column < P.width) // (J) Kiá»ƒm tra biÃªn cho P 

    P.elements[row*P.width+column] = pValue; // (K) Ghi káº¿t quáº£â€ 

VÃ²ng láº·pâ€¯iâ€¯(vÃ²ng láº·p qua cÃ¡c "phase" hay cÃ¡c cáº·p khá»‘i con cá»§a M vÃ  N) cháº¡y Ä‘á»ƒ tÃ­ch lÅ©y giÃ¡ trá»‹â€¯pValueâ€¯hoÃ n chá»‰nh cho pháº§n tá»­ P[row][column]. 

(J):â€¯Sau khi vÃ²ng láº·pâ€¯iâ€¯káº¿t thÃºc,â€¯pValueâ€¯chá»©a giÃ¡ trá»‹ cuá»‘i cÃ¹ng cho pháº§n tá»­ P[row][column]. Má»™t láº§n ná»¯a, kiá»ƒm tra biÃªn Ä‘Æ°á»£c thá»±c hiá»‡n Ä‘á»ƒ Ä‘áº£m báº£o luá»“ng chá»‰ ghi vÃ o cÃ¡c vá»‹ trÃ­ há»£p lá»‡ cá»§a ma tráº­n P. 

(K):â€¯Náº¿u trong biÃªn, giÃ¡ trá»‹â€¯pValueâ€¯(lÆ°u trong thanh ghi cá»§a luá»“ng) Ä‘Æ°á»£c ghi ra vá»‹ trÃ­ tÆ°Æ¡ng á»©ngâ€¯P[row*P.width+column]â€¯trong global memory. 
 

Code CUDA kernel  

__global__ void MatrixMulKernel(Matrix M, Matrix N, Matrix P) 

{ 

__shared__ float tileMs[TILE_WIDTH][TILE_WIDTH]; 

__shared__ float tileNs[TILE_WIDTH][TILE_WIDTH]; 

 

int tx = threadIdx.x; int ty = threadIdx.y; 

int bx = blockIdx.x; int by = blockIdx.y; 

 

int row = by * TILE_WIDTH + ty; 

int column = bx * TILE_WIDTH + tx; 

 

float pValue = 0; 

 

for(int i=0;i<ceilf(M.width/(float)TILE_WIDTH);i++){ 

if(row < M.height && (i*TILE_WIDTH + tx)<M.width) 

tileMs[ty][tx] = M.elements[row*M.width + i*TILE_WIDTH + tx]; 

else 

tileMs[ty][tx] = 0; 

if(column < N.width && (i*TILE_WIDTH + ty)<N.height) 

tileNs[ty][tx] = N.elements[(i*TILE_WIDTH + ty)*N.width + column]; 

else 

tileNs[ty][tx] = 0; 

 

__syncthreads(); 

 

for(int j=0;j<TILE_WIDTH;j++) 

pValue += tileMs[ty][j] * tileNs[j][tx]; 

 

__syncthreads(); 

} 

if(row < P.height && column < P.width) 

P.elements[row*P.width+column] = pValue; 

} 

 

HÃ m Host gá»i Kernel (MatrixMulOnDeviceâ€¯trongâ€¯matrixmul.cu) 

HÃ m nÃ y chá»‹u trÃ¡ch nhiá»‡m quáº£n lÃ½ dá»¯ liá»‡u, cáº¥u hÃ¬nh vÃ  khá»Ÿi cháº¡y kernel trÃªn GPU. 

 

HÃ¬nh 4.2 SÆ¡ Ä‘á»“ thuáº­t toÃ¡n song song pháº§n Host 

Cáº¥p phÃ¡t bá»™ nhá»› trÃªn Host (CPU) vÃ  Device (GPU): 

Host:â€¯HÃ mâ€¯AllocateMatrixâ€¯(trongâ€¯main) cáº¥p phÃ¡t bá»™ nhá»› cho M, N, P trÃªn CPU báº±ngâ€¯malloc. 

Device:â€¯HÃ mâ€¯AllocateDeviceMatrixâ€¯(Ä‘Æ°á»£c gá»i trongâ€¯MatrixMulOnDevice) cáº¥p phÃ¡t bá»™ nhá»› trÃªn GPU cho Md, Nd, Pd báº±ngâ€¯cudaMalloc. 

â€œMatrix Md = AllocateDeviceMatrix(M); 

... 

Matrix Nd = AllocateDeviceMatrix(N); 

... 

Matrix Pd = AllocateDeviceMatrix(P);â€ 

Sao chÃ©p dá»¯ liá»‡u tá»« Host sang Device (cudaMemcpyHostToDevice): 

HÃ mâ€¯CopyToDeviceMatrixâ€¯Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ sao chÃ©p dá»¯ liá»‡u tá»« ma tráº­n M, N trÃªn host sang Md, Nd trÃªn device. 

â€œCopyToDeviceMatrix(Md, M); 

  CopyToDeviceMatrix(Nd, N);â€ 

BÃªn trongâ€¯CopyToDeviceMatrix: 

â€œcudaMemcpy(Mdevice.elements, Mhost.elements, size, cudaMemcpyHostToDevice);â€ 

Cáº¥u hÃ¬nh Grid vÃ  Block: 

â€œdim3 gridSize, blockSize; 

blockSize.x = blockSize.y = TILE_WIDTH; blockSize.z = 1; 

gridSize.x = ceil(P.width/(float)blockSize.x); 

gridSize.y = ceil(P.height/(float)blockSize.y); 

gridSize.z = 1;â€ 

blockSize: KÃ­ch thÆ°á»›c cá»§a má»—i khá»‘i luá»“ng Ä‘Æ°á»£c Ä‘áº·t lÃ â€¯TILE_WIDTHâ€¯xâ€¯TILE_WIDTHâ€¯ 

gridSize: KÃ­ch thÆ°á»›c cá»§a grid (sá»‘ lÆ°á»£ng khá»‘i luá»“ng) Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn kÃ­ch thÆ°á»›c cá»§a ma tráº­n P vÃ â€¯TILE_WIDTH, Ä‘áº£m báº£o cÃ³ Ä‘á»§ khá»‘i luá»“ng Ä‘á»ƒ bao phá»§ toÃ n bá»™ ma tráº­n P. HÃ mâ€¯ceilâ€¯Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ lÃ m trÃ²n lÃªn. 

Gá»i Kernel: 

â€œMatrixMulKernel<<<gridSize, blockSize>>>(Md, Nd, Pd);â€ 

ÄÃ¢y lÃ  lá»‡nh khá»Ÿi cháº¡y kernelâ€¯MatrixMulKernelâ€¯trÃªn GPU.â€¯gridSizeâ€¯vÃ â€¯blockSizeâ€¯xÃ¡c Ä‘á»‹nh cáº¥u hÃ¬nh thá»±c thi.â€¯Md,â€¯Nd,â€¯Pdâ€¯lÃ  cÃ¡c con trá» tá»›i dá»¯ liá»‡u ma tráº­n trÃªn device. 

Äá»“ng bá»™ hÃ³a (cudaDeviceSynchronize()â€¯hoáº·c thÃ´ng quaâ€¯cudaEventSynchronize): 

Trong Ä‘oáº¡n code nÃ y, viá»‡c Ä‘á»“ng bá»™ hÃ³a Ä‘Æ°á»£c thá»±c hiá»‡n ngáº§m thÃ´ng quaâ€¯cudaEventSynchronize(stop);â€¯sau khi Ä‘o thá»i gian. Lá»‡nh nÃ y Ä‘áº£m báº£o ráº±ng CPU sáº½ Ä‘á»£i cho Ä‘áº¿n khi kernelâ€¯MatrixMulKernelâ€¯(vÃ  cÃ¡c lá»‡nh CUDA trÆ°á»›c Ä‘Ã³ liÃªn quan Ä‘áº¿n eventâ€¯stop) hoÃ n thÃ nh viá»‡c thá»±c thi trÃªn GPU. 
â€œcudaEventRecord(stop, 0); 

cudaEventSynchronize(stop); // Chá» kernel hoÃ n thÃ nhâ€ 

Sao chÃ©p káº¿t quáº£ tá»« Device vá» Host (cudaMemcpyDeviceToHost): 

Sau khi kernel hoÃ n thÃ nh, hÃ mâ€¯CopyFromDeviceMatrixâ€¯Ä‘Æ°á»£c gá»i Ä‘á»ƒ sao chÃ©p ma tráº­n káº¿t quáº£ Pd tá»« device vá» ma tráº­n P trÃªn host. 

â€œCopyFromDeviceMatrix(P, Pd);â€ 

BÃªn trongâ€¯CopyFromDeviceMatrix:  

â€œcudaMemcpy(Mhost.elements, Mdevice.elements, size, cudaMemcpyDeviceToHost);â€ 

Giáº£i phÃ³ng bá»™ nhá»›: 

Device:â€¯HÃ mâ€¯FreeDeviceMatrixâ€¯Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ giáº£i phÃ³ng bá»™ nhá»› Ä‘Ã£ cáº¥p phÃ¡t trÃªn GPU cho Md, Nd, Pd báº±ngâ€¯cudaFree. 

â€œFreeDeviceMatrix(&Md); 

 FreeDeviceMatrix(&Nd); 

 FreeDeviceMatrix(&Pd);â€ 

Host:â€¯HÃ mâ€¯FreeMatrixâ€¯(trongâ€¯main) giáº£i phÃ³ng bá»™ nhá»› Ä‘Ã£ cáº¥p phÃ¡t trÃªn CPU cho M, N, P báº±ngâ€¯free. 

Code Host gá»i Kernel 

void MatrixMulOnDevice(const Matrix M, const Matrix N, Matrix P) 

{ 

    Matrix Md = AllocateDeviceMatrix(M); 

    CopyToDeviceMatrix(Md, M); 

    Matrix Nd = AllocateDeviceMatrix(N); 

    CopyToDeviceMatrix(Nd, N); 

 

    Matrix Pd = AllocateDeviceMatrix(P); 

    CopyToDeviceMatrix(Pd, P);  

 

    dim3 gridSize, blockSize; 

    blockSize.x = blockSize.y = TILE_WIDTH; blockSize.z = 1; 

    gridSize.x = ceil(P.width/(float)blockSize.x); 

    gridSize.y = ceil(P.height/(float)blockSize.y); 

    gridSize.z = 1; 

 

    cudaEvent_t start, stop; 

    cudaEventCreate(&start); 

    cudaEventCreate(&stop); 

    cudaEventRecord(start, 0); 

 

    MatrixMulKernel<<<gridSize, blockSize>>>(Md, Nd, Pd); 

 

    cudaEventRecord(stop, 0); 

    cudaEventSynchronize(stop); 

 

    float gpu_time = 0.0f; 

    cudaEventElapsedTime(&gpu_time, start, stop); 

    printf("ğŸš€ GPU Time: %.4f ms\n", gpu_time); 

 

    cudaEventDestroy(start); 

    cudaEventDestroy(stop); 

 

    CopyFromDeviceMatrix(P, Pd);  

 

    FreeDeviceMatrix(&Md); 

    FreeDeviceMatrix(&Nd); 

    FreeDeviceMatrix(&Pd); 

} 
